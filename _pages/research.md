---
layout: archive
title: ""
permalink: /research/
author_profile: true
---

{% include base_path %}

# Finance

<!--   <div style="background-color:#D3D3D3;padding:20px;"> -->
# Machine Learning

* Bandit Algorithms for Factorial Experiments  
  [PDF](https://yutongyan.xyz/files/bandits_for_factorial_report.pdf) &nbsp;&nbsp;&nbsp; [Poster](https://yutongyan.xyz/files/bandits_for_factorial_poster.pdf) &nbsp;&nbsp;&nbsp;  [Slides](https://yutongyan.xyz/files/bandits_for_factorial_slides.pdf)
  <details open>
    <summary>Abstract</summary>
<!--     <div class="language-plaintext highlighter-rouge"> -->
          This is abstract.   
<!--     </div> -->
  </details>

* A Theoretical Analysis of Upper Confidence Bound applied to Trees  
  [PDF](https://yutongyan.xyz/files/uct_proof.pdf) &nbsp;&nbsp;&nbsp; [Slides](https://yutongyan.xyz/files/uct_slides.pdf)  
  <details open>
    <summary>Abstract</summary>
          This is abstract.   
  </details>
  
* Improving Reward Shaping via Language Instruction 
  [PDF](https://yutongyan.xyz/files/rl_nlp_report.pdf) &nbsp;&nbsp;&nbsp; [Slides](https://yutongyan.xyz/files/rl_nlp_slides.pdf)  
  <details open>
    <summary>Abstract</summary>
      Recent literature have started exploring problems in language-assisted reinforcement learning (RL). This is a setting where language itself is not necessary, but incorporating language can be advantageous for solving the task. In this work, We explore one specific case of language-assisted RL, where we address the challenges of solving tasks with sparse reward. We do so by using reward shaping through leveraging natural language instructions. Our project extends a recent work, in which a framework called LanguagE-Action Reward Network (LEARN) is proposed. LEARN maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. The work has shown that language-based rewards can lead to performance advancement compared to learning without language through extensive experiments. However, one draw back of the method is that the reward shaping is only dependent on the actions for simplicity. In this work, we would like to extend the approach by learning a state representation using a VAE, and conduct reward shaping based on both actions and the state representation. Our empirical results show that the VAE is not able to learn representations that can significantly improve baselines, but other methods of representation learning for states should be investigated in future work.
  </details>



<!--
{% for post in site.writing-sample reversed %}
  {% include archive-single.html %}
{% endfor %}
-->
